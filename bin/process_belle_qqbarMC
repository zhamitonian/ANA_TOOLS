#!/usr/bin/env python3
"""
Belle Generic MC Processing Job Submission Tool

Processes Belle I generic (qqbar) Monte Carlo data for different event types
(evtgen-charged, evtgen-mixed, evtgen-charm, evtgen-uds, etc.).

Supports both basf and basf2 (b2bii) frameworks.

Author: GitHub Copilot
Date: December 5, 2025
version: 2.1
"""

import argparse
import subprocess
import sys
import os
from typing import List, Set, Optional, Dict, Tuple
from belle_run_manager import BelleRunManager, RunEntry

class BelleMCProcessor:
    """
    Belle generic MC processing job submission manager
    
    Handles:
    - Job submission to LSF batch system
    - Support for basf and b2bii frameworks
    - Multiple event types and stream processing
    - Integration with BelleRunManager for run information
    """
    
    # Mapping: URL dataset name -> list of database dataset names
    DATASET_URL_MAPPING = {
        "1S_scan" : ["1S", "1S_offres", "1S_scan"],
        "2S_scan" : ["2S", "2S_offres", "2S_scan"],
        "3S_scan" : ["3S", "3S_offres", "3S_scan"],
        "on_resonance" : ["4S"],
        "continuum" : ["4S_offres"],
        "5S_scan" : ["5S_offres", "5S_scan"],
        "5S_on_resonance" : ["5S"] 
    }
    
    # Event types available for generic MC
    EVENT_TYPES = {
        "1S_scan": ["evtgen-charm", "evtgen-uds"],
        "2S_scan": ["evtgen-charm", "evtgen-uds"],
        "3S_scan": ["evtgen-charm", "evtgen-uds"],
        "on_resonance": ["evtgen-charged", "evtgen-mixed", "evtgen-charm", "evtgen-uds"],
        "continuum": ["evtgen-charm", "evtgen-uds"],
        "5S_on_resonance": ["evtgen-charm", "evtgen-uds", "evtgen-bsbs", "evtgen-nonbsbs"],
        "5S_scan": ["evtgen-charm", "evtgen-uds"]
    }

    def __init__(self, run_manager: BelleRunManager, framework: str = 'b2bii'):
        """
        Initialize MC processor
        
        Args:
            run_manager: BelleRunManager instance for run information
            framework: 'basf' or 'b2bii'
        """
        self.run_manager = run_manager
        self.framework = framework
    
    def _get_url_dataset(self, dataset: str) -> str:
        """Convert database dataset name to URL dataset name"""
        for url_name, db_names in self.DATASET_URL_MAPPING.items():
            if dataset in db_names:
                return url_name
        return dataset  # fallback to original if not found
    
    def _submit_b2bii_job(self, exp: int, run_start: int, run_end: int,
                         event_type: str, data_type: str, stream_no: int,
                         output_dir: str, script_path: str) -> None:
        """Submit b2bii job for generic MC"""
        log_path = os.path.join(output_dir, "log", data_type)
        root_path = os.path.join(output_dir, data_type)
        os.makedirs(log_path, exist_ok=True)
        os.makedirs(root_path, exist_ok=True)
        
        job_name = f"{event_type}_e{exp}r{run_start}_{run_end}_st{stream_no}"
        url_dataset = self._get_url_dataset(data_type)
        url = f"http://bweb3/montecarlo.php?ex={exp}&rs={run_start}&re={run_end}&ty={event_type}&dt={url_dataset}&bl=caseB&st={stream_no}"
        root_file = os.path.join(root_path, f"{job_name}.root")
        log_file = os.path.join(log_path, f"{job_name}.log")
        
        cmd = f'bsub -q s -oo {log_file} basf2 {script_path} belle1 gMC "{url}" {root_file}'
        print(f"Submitting: {cmd}")
        subprocess.run(cmd, shell=True, check=True)
    
    def _submit_basf_job(self, exp: int, run_start: int, run_end: int,
                        event_type: str, data_type: str, stream_no: int,
                        output_dir: str, so_file: str) -> None:
        """Submit basf job for generic MC"""
        log_path = os.path.join(output_dir, "log", data_type)
        root_path = os.path.join(output_dir, data_type)
        script_path = os.path.join(output_dir, "scripts")
        os.makedirs(log_path, exist_ok=True)
        os.makedirs(root_path, exist_ok=True)
        os.makedirs(script_path, exist_ok=True)
        
        job_name = f"{event_type}_e{exp}r{run_start}_{run_end}_st{stream_no}"
        url_dataset = self._get_url_dataset(data_type)
        url = f"http://bweb3/montecarlo.php?ex={exp}&rs={run_start}&re={run_end}&ty={event_type}&dt={url_dataset}&bl=caseB&st={stream_no}"
        root_file = os.path.join(root_path, f"{job_name}.root")
        log_file = os.path.join(log_path, f"{job_name}.log")
        
        # Create shell script
        shell_script = os.path.join(script_path, f"{job_name}.sh")
        self._create_basf_script(shell_script, url, root_file, so_file)
        
        cmd = f"bsub -q s -cwd {output_dir} -oo {log_file} /sw/belle/local/bin/centos7-exec bash {shell_script}"
        print(f"Submitting: {cmd}")
        subprocess.run(cmd, shell=True, check=True)
    
    def _create_basf_script(self, filename: str, url: str, 
                           output_root: str, so_file: str) -> None:
        """Create BASF shell script for job submission"""
        module_filename = os.path.basename(so_file)
        module, _ = os.path.splitext(module_filename)

        with open(filename, 'w') as f:
            f.write("#!/bin/bash\n")
            f.write('source /sw/belle/local/etc/bashrc_general\n')
            f.write('export BASF_USER_IF=basfsh.so\n')
            f.write('export BASF_USER_INIT=user_init.so\n\n')
            f.write('basf << EOF\n\n')
            f.write(f'module register fix_mdst {module}\n')
            f.write('path create main\n')
            f.write('path create Analysis\n\n')
            f.write(f'path add_module main fix_mdst {module}\n\n')
            f.write('path add_condition main >:0:Analysis\n')
            f.write('path add_condition main =<:0:KILL\n')
            f.write(f'path add_module Analysis {module}\n\n')
            f.write(f'module put_parameter {module} output_filename\\{output_root}\n')
            f.write(f'module put_parameter {module} isMCSample\\1\n')
            f.write(f'module put_parameter {module} rmMCTree\\1\n')
            f.write(f'module put_parameter {module} rmTree\\0\n')
            f.write(f'module put_parameter {module} rmMixTree\\1\n\n')
            f.write('initialize\n')
            f.write(f'process_url {url}\n\n')
            f.write('terminate\n')
            f.write('EOF\n')
            f.write('exit\n')
        
        os.chmod(filename, 0o755)
    
    def submit_jobs(self, script_path: str, selected_datasets: Optional[Set[str]],
                   selected_exps: Optional[Set[int]], event_types: Optional[List[str]],
                   streams: range, output_dir: str, runs_per_job: int) -> int:
        """
        Submit processing jobs for generic MC
        
        Args:
            script_path: Path to analysis script
            selected_datasets: Set of dataset types to process (None for all)
            selected_exps: Set of experiments to process (None for all)
            event_types: List of event types to process (None for all available)
            streams: Stream numbers to process
            output_dir: Output directory for results
            runs_per_job: Number of runs per job
            
        Returns:
            Total number of jobs submitted
        """
        # Get selected runs from BelleRunManager
        selected_runs = self.run_manager.filter_runs(
            experiments=selected_exps,
            datasets=selected_datasets
        )
        
        if not selected_runs:
            print("Error: No runs match the specified criteria")
            sys.exit(1)
        
        # Group runs by (experiment, dataset)
        grouped_runs = self.run_manager.group_by_exp_dataset(selected_runs)
        
        print(f"\nFound {len(selected_runs)} runs matching criteria")
        print(f"Grouped into {len(grouped_runs)} (experiment, dataset) combinations\n")
        
        # Get statistics
        stats = self.run_manager.get_statistics(selected_runs)
        print(f"Total luminosity: {stats['total_luminosity']:.2f} pb⁻¹ ({stats['total_luminosity']/1000:.3f} fb⁻¹)")
        
        # Submit jobs
        total_jobs = 0
        for (exp, dataset), runs in sorted(grouped_runs.items()):
            # Get URL dataset name for event type lookup
            url_dataset = self._get_url_dataset(dataset)
            
            # Determine event types for this dataset
            if event_types:
                valid_event_types = [et for et in event_types if et in self.EVENT_TYPES.get(url_dataset, [])]
                if not valid_event_types:
                    print(f"Warning: None of the specified event types are valid for {dataset}")
                    continue
            else:
                valid_event_types = self.EVENT_TYPES.get(url_dataset, [])
            
            if not valid_event_types:
                print(f"Warning: No event types defined for dataset {dataset} (URL: {url_dataset})")
                continue
            
            print(f"\nProcessing Exp {exp}, Dataset {dataset}: {len(runs)} runs")
            print(f"  Event types: {', '.join(valid_event_types)}")
            print(f"  Streams: {', '.join(str(s) for s in streams)}")
            print(f"  Runs per job: {runs_per_job}")
            
            # Group consecutive runs
            consecutive_groups = []
            current_group = [runs[0]]
            
            for i in range(1, len(runs)):
                if runs[i] == current_group[-1] + 1:
                    current_group.append(runs[i])
                else:
                    consecutive_groups.append(current_group)
                    current_group = [runs[i]]
            consecutive_groups.append(current_group)
            
            print(f"  Split into {len(consecutive_groups)} consecutive run groups")
            
            # Submit jobs for each consecutive group
            for group in consecutive_groups:
                for event_type in valid_event_types:
                    for stream_no in streams:
                        # Add extra stream offset for old experiments
                        extra_stream = 10 if exp < 30 else 0
                        if exp < 30 and dataset == "4S_offres":  # continuum
                            extra_stream += 10
                        adjusted_stream = stream_no + extra_stream
                        
                        # Submit jobs in batches
                        for i in range(0, len(group), runs_per_job):
                            batch_runs = group[i:i + runs_per_job]
                            run_start = batch_runs[0]
                            run_end = batch_runs[-1]
                            
                            if self.framework == 'basf':
                                self._submit_basf_job(exp, run_start, run_end, event_type,
                                                    dataset, adjusted_stream, output_dir, script_path)
                            else:  # b2bii
                                self._submit_b2bii_job(exp, run_start, run_end, event_type,
                                                     dataset, adjusted_stream, output_dir, script_path)
                            
                            total_jobs += 1
        
        return total_jobs


def main():
    parser = argparse.ArgumentParser(
        description="Process Belle generic MC data using run configurations",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Process all generic MC data with default settings
  %(prog)s script.py
  
  # Process on_resonance data only
  %(prog)s script.py --data-type on_resonance
  
  # Process specific data types for experiment 7
  %(prog)s script.py --data-type on_resonance continuum --exp 7
  
  # Process with specific event types
  %(prog)s script.py --event-type evtgen-charm evtgen-uds
  
  # Process with 2 streams
  %(prog)s script.py --streams 2
  
  # List available data types and configurations
  %(prog)s --list
        """
    )
    
    parser.add_argument("script_path", nargs='?',
                       help="Path to the analysis script (.py for b2bii, .so for basf)")
    parser.add_argument("--framework", "-fw", type=str, choices=['basf', 'b2bii'], 
                       default='b2bii', help="Choose analysis framework (default: b2bii)")
    
    # Filter options
    filter_group = parser.add_argument_group('Filters')
    filter_group.add_argument("--dataset", "-d", type=str, nargs='+',
                             help="Filter by dataset type(s) (e.g., 4S, 4S_offres, 5S). If not specified, process all datasets")
    filter_group.add_argument("--exp", type=int, nargs='+',
                             help="Filter by experiment number(s)")
    filter_group.add_argument("--event-type", "-e", type=str, nargs='+',
                             choices=["evtgen-charged", "evtgen-mixed", "evtgen-charm", 
                                     "evtgen-uds", "evtgen-bsbs", "evtgen-nonbsbs"],
                             help="Event types to process (default: all available for dataset)")
    filter_group.add_argument("--list", action="store_true",
                             help="List available datasets and experiments")
    filter_group.add_argument("--db", type=str,
                             default=os.path.expanduser("~/.config/belle/run_database_energy.dat"),
                             help="Path to run database file (default: ~/.config/belle/run_database_energy.dat)")
    
    # Configuration options
    config_group = parser.add_argument_group('Configuration')
    config_group.add_argument("--streams", "-s", type=int, default=4,
                             help="Number of streams to process (default: 4, range: 0 to streams-1)")
    config_group.add_argument("--runs-per-job", "-r", type=int, default=100,
                             help="Number of runs per job (default: 10)")
    config_group.add_argument("--output-dir", type=str, default=None,
                             help="Output directory (default: same as script directory)")
    
    args = parser.parse_args()
    
    # Initialize BelleRunManager
    run_manager = BelleRunManager(args.db)
    run_manager.load_database()
    
    # Initialize processor
    processor = BelleMCProcessor(run_manager, args.framework)
    
    # List datasets if requested
    if args.list:
        run_manager.print_summary()
        sys.exit(0)
    
    # Verify script path
    if not args.script_path:
        print("Error: script_path is required for job submission")
        parser.print_help()
        sys.exit(1)
    
    script_path = os.path.abspath(args.script_path)
    if not os.path.exists(script_path):
        print(f"Error: Script not found: {script_path}")
        sys.exit(1)
    
    # Set output directory to script's base directory if not specified
    if args.output_dir is None:
        args.output_dir = os.path.dirname(script_path)
    
    # Get datasets from command line (None means all datasets)
    selected_datasets = set(args.dataset) if args.dataset else None
    
    # Filter experiments
    selected_exps = set(args.exp) if args.exp else None
    
    # Determine stream numbers to process
    streams = range(args.streams)
    
    # Submit jobs
    total_jobs = processor.submit_jobs(
        script_path=script_path,
        selected_datasets=selected_datasets,
        selected_exps=selected_exps,
        event_types=args.event_type,
        streams=streams,
        output_dir=args.output_dir,
        runs_per_job=args.runs_per_job
    )
    
    print(f"\n{'='*60}")
    print(f"✓ Submitted {total_jobs} jobs successfully")
    print(f"Output directory: {args.output_dir}")
    print(f"Log files: {args.output_dir}/log/<dataset>")
    print(f"Root files: {args.output_dir}/<dataset>")
    print(f"{'='*60}\n")

        


if __name__ == "__main__":
    main()
