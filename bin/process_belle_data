#!/usr/bin/env python3
"""
Belle Data Processing Job Submission Tool

Uses BelleRunManager to handle experiment run information and submits
analysis jobs to the batch system (LSF) for Belle I data processing.

Supports both basf and basf2 (b2bii) frameworks.

Author: GitHub Copilot
Date: December 5, 2025
version: 2.1
"""

import argparse
import subprocess
import sys
import os
from typing import List, Set, Optional, Dict, Tuple
from belle_run_manager import BelleRunManager, RunEntry


class BelleDataProcessor:
    """
    Belle data processing job submission manager
    
    Handles:
    - Job submission to LSF batch system
    - Support for basf and b2bii frameworks
    - Integration with BelleRunManager for run information
    """
    
    # Mapping: URL dataset name -> list of database dataset names
    DATASET_URL_MAPPING = {
        "1S_scan" : ["1S", "1S_offres", "1S_scan"],
        "2S_scan" : ["2S", "2S_offres", "2S_scan"],
        "3S_scan" : ["3S", "3S_offres", "3S_scan"],
        "on_resonance" : ["4S"],
        "continuum" : ["4S_offres"],
        "5S_scan" : ["5S_offres", "5S_scan"],
        "5S_on_resonance" : ["5S"] 
    }

    def __init__(self, run_manager: BelleRunManager, framework: str = 'b2bii'):
        """
        Initialize data processor
        
        Args:
            run_manager: BelleRunManager instance for run information
            framework: 'basf' or 'b2bii'
        """
        self.run_manager = run_manager
        self.framework = framework
    
    def _get_url_dataset(self, dataset: str) -> str:
        """Convert database dataset name to URL dataset name"""
        for url_name, db_names in self.DATASET_URL_MAPPING.items():
            if dataset in db_names:
                return url_name
        return dataset  # fallback to original if not found
    
    def _submit_b2bii_job(self, exp: int, run_start: int, run_end: int,
                         skim_type: str, dataset: str, output_dir: str, 
                         script_path: str) -> None:
        """Submit b2bii job for a run range"""
        log_path = os.path.join(output_dir, "log", dataset)
        root_path = os.path.join(output_dir, dataset)
        os.makedirs(log_path, exist_ok=True)
        os.makedirs(root_path, exist_ok=True)
        
        job_name = f"{skim_type}_e{exp}r{run_start}_{run_end}"
        url_dataset = self._get_url_dataset(dataset)
        url = f"http://bweb3/mdst.php?ex={exp}&rs={run_start}&re={run_end}&skm={skim_type}&dt={url_dataset}&bl=caseB"
        root_file = os.path.join(root_path, f"{job_name}.root")
        log_file = os.path.join(log_path, f"{job_name}.log")
        
        cmd = f'bsub -q h -oo {log_file} basf2 {script_path} belle1 data "{url}" {root_file}'
        print(f"Submitting: {cmd}")
        subprocess.run(cmd, shell=True, check=True)
    
    def _submit_basf_job(self, exp: int, run_start: int, run_end: int,
                        skim_type: str, dataset: str, output_dir: str,
                        so_file: str) -> None:
        """Submit basf job for a run range"""
        log_path = os.path.join(output_dir, "log", dataset)
        root_path = os.path.join(output_dir, dataset)
        script_path = os.path.join(output_dir, "scripts")
        os.makedirs(log_path, exist_ok=True)
        os.makedirs(root_path, exist_ok=True)
        os.makedirs(script_path, exist_ok=True)
        
        job_name = f"{skim_type}_e{exp}r{run_start}_{run_end}"
        url_dataset = self._get_url_dataset(dataset)
        url = f"http://bweb3/mdst.php?ex={exp}&rs={run_start}&re={run_end}&skm={skim_type}&dt={url_dataset}&bl=caseB"
        root_file = os.path.join(root_path, f"{job_name}.root")
        log_file = os.path.join(log_path, f"{job_name}.log")
        
        # Create shell script
        shell_script = os.path.join(script_path, f"{job_name}.sh")
        self._create_basf_script(shell_script, url, root_file, so_file)
        
        cmd = f"bsub -q s -cwd {output_dir} -oo {log_file} /sw/belle/local/bin/centos7-exec bash {shell_script}"
        print(f"Submitting: {cmd}")
        subprocess.run(cmd, shell=True, check=True)
    
    def _create_basf_script(self, filename: str, url: str, 
                           output_root: str, so_file: str) -> None:
        """Create BASF shell script for job submission"""
        module_filename = os.path.basename(so_file)
        module, _ = os.path.splitext(module_filename)

        with open(filename, 'w') as f:
            f.write("#!/bin/bash\n")
            f.write('source /sw/belle/local/etc/bashrc_general\n')
            f.write('export BASF_USER_IF=basfsh.so\n')
            f.write('export BASF_USER_INIT=user_init.so\n\n')
            f.write('basf << EOF\n\n')
            f.write(f'module register fix_mdst {module}\n')
            f.write('path create main\n')
            f.write('path create Analysis\n\n')
            f.write(f'path add_module main fix_mdst {module}\n\n')
            f.write('path add_condition main >:0:Analysis\n')
            f.write('path add_condition main =<:0:KILL\n')
            f.write(f'path add_module Analysis {module}\n\n')
            f.write(f'module put_parameter {module} output_filename\\{output_root}\n')
            f.write(f'module put_parameter {module} isMCSample\\0\n\n')
            f.write('initialize\n')
            f.write(f'process_url {url}\n\n')
            f.write('terminate\n')
            f.write('EOF\n')
            f.write('exit\n')
        
        os.chmod(filename, 0o755)
    
    def submit_jobs(self, script_path: str, selected_datasets: Set[str],
                   selected_exps: Optional[Set[int]], skim_type: str,
                   output_dir: str, runs_per_job: int) -> int:
        """
        Submit processing jobs for selected runs
        
        Args:
            script_path: Path to analysis script
            selected_datasets: Set of dataset types to process
            selected_exps: Set of experiments to process (None for all)
            skim_type: Skim type to use
            output_dir: Output directory for results
            runs_per_job: Number of runs per job
            
        Returns:
            Total number of jobs submitted
        """
        # Get selected runs from BelleRunManager
        selected_runs = self.run_manager.filter_runs(
            experiments=selected_exps,
            datasets=selected_datasets
        )
        
        if not selected_runs:
            print("Error: No runs match the specified criteria")
            sys.exit(1)
        
        # Group runs by (experiment, dataset)
        grouped_runs = self.run_manager.group_by_exp_dataset(selected_runs)
        
        print(f"\nFound {len(selected_runs)} runs matching criteria")
        print(f"Grouped into {len(grouped_runs)} (experiment, dataset) combinations\n")
        
        # Get statistics
        stats = self.run_manager.get_statistics(selected_runs)
        print(f"Total luminosity: {stats['total_luminosity']:.2f} pb⁻¹ ({stats['total_luminosity']/1000:.3f} fb⁻¹)")
        
        # Submit jobs
        total_jobs = 0
        for (exp, dataset), runs in sorted(grouped_runs.items()):
            dataset_label = dataset
            
            print(f"\nProcessing Exp {exp}, Dataset {dataset}: {len(runs)} runs")
            print(f"  Runs per job: {runs_per_job}")
            
            # Group consecutive runs to avoid mixing different datasets
            # Only merge runs that are consecutive in the run number
            consecutive_groups = []
            current_group = [runs[0]]
            
            for i in range(1, len(runs)):
                if runs[i] == current_group[-1] + 1:
                    current_group.append(runs[i])
                else:
                    consecutive_groups.append(current_group)
                    current_group = [runs[i]]
            consecutive_groups.append(current_group)
            
            print(f"  Split into {len(consecutive_groups)} consecutive run groups")
            
            # Submit jobs for each consecutive group
            for group in consecutive_groups:
                for i in range(0, len(group), runs_per_job):
                    batch_runs = group[i:i + runs_per_job]
                    run_start = batch_runs[0]
                    run_end = batch_runs[-1]
                    
                    if self.framework == 'basf':
                        self._submit_basf_job(exp, run_start, run_end, skim_type, 
                                            dataset_label, output_dir, script_path)
                    else:  # b2bii
                        self._submit_b2bii_job(exp, run_start, run_end, skim_type,
                                             dataset_label, output_dir, script_path)
                    
                    total_jobs += 1
        
        return total_jobs


def main():
    parser = argparse.ArgumentParser(
        description="Process Belle data using run database",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Process all datasets with default settings
  %(prog)s script.py
  
  # Process Y(4S) data only with 50 runs per job
  %(prog)s script.py --dataset 4S --runs-per-job 50
  
  # Process specific datasets for experiment 7
  %(prog)s script.py --dataset 4S 4S_offres --exp 7
  
  # Process all data for specific experiments
  %(prog)s script.py --exp 7 11 13
  
  # List available datasets and experiments
  %(prog)s --list
        """
    )
    
    parser.add_argument("script_path", nargs='?',
                       help="Path to the analysis script (.py for b2bii, .so for basf)")
    parser.add_argument("--framework", "-fw", type=str, choices=['basf', 'b2bii'], 
                       default='b2bii', help="Choose analysis framework (default: b2bii)")
    
    # Filter options
    filter_group = parser.add_argument_group('Filters')
    filter_group.add_argument("--dataset", "-d", type=str, nargs='+',
                             help="Filter by dataset type(s) (e.g., 4S, 4S_offres, 5S). If not specified, process all datasets")
    filter_group.add_argument("--exp", type=int, nargs='+', 
                             help="Filter by experiment number(s)")
    filter_group.add_argument("--list", action="store_true", 
                             help="List available datasets and experiments")
    filter_group.add_argument("--db", type=str, 
                             default=os.path.expanduser("~/.config/belle/run_database_energy.dat"),
                             help="Path to run database file (default: ~/.config/belle/run_database_energy.dat)")
    
    # Configuration options
    config_group = parser.add_argument_group('Configuration')
    config_group.add_argument("--skim-type", type=str, default="HadronBorJ",
                            help="Skim type to use (default: HadronBorJ)")
    config_group.add_argument("--runs-per-job", "-r", type=int, default=100,
                            help="Number of runs per job (required)")
    config_group.add_argument("--output-dir", type=str, default=None,
                            help="Output directory (default: same as script directory)")
    
    args = parser.parse_args()
    
    # Initialize BelleRunManager
    run_manager = BelleRunManager(args.db)
    run_manager.load_database()
    
    # List datasets if requested
    if args.list:
        run_manager.print_summary()
        sys.exit(0)
    
    # Verify script path
    if not args.script_path:
        print("Error: script_path is required for job submission")
        parser.print_help()
        sys.exit(1)
    
    script_path = os.path.abspath(args.script_path)
    if not os.path.exists(script_path):
        print(f"Error: Script not found: {script_path}")
        sys.exit(1)
    
    # Set output directory to script's base directory if not specified
    if args.output_dir is None:
        args.output_dir = os.path.dirname(script_path)
    
    # Get datasets from command line (None means all datasets)
    selected_datasets = set(args.dataset) if args.dataset else None
    
    # Filter experiments
    selected_exps = set(args.exp) if args.exp else None
    
    # Initialize processor and submit jobs
    processor = BelleDataProcessor(run_manager, args.framework)
    total_jobs = processor.submit_jobs(
        script_path=script_path,
        selected_datasets=selected_datasets,
        selected_exps=selected_exps,
        skim_type=args.skim_type,
        output_dir=args.output_dir,
        runs_per_job=args.runs_per_job
    )
    
    print(f"\n{'='*60}")
    print(f"✓ Submitted {total_jobs} jobs successfully")
    print(f"Output directory: {args.output_dir}")
    print(f"Log files: {args.output_dir}/log/<dataset>")
    print(f"Root files: {args.output_dir}/<dataset>")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    main()


"""
version history:
- v2.0 (Nov 4, 2025): change dataset reading from v1.0 to use belle_run_manager
- v2.1 (Dec 5, 2025): 
    - Fixed subprocess.run calls to ensure jobs are submitted correctly.
    - Improved dataset URL mapping logic.
    - Enhanced script documentation and examples.
"""